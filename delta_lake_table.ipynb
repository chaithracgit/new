{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "274c9ecf-47c3-4315-8cdd-15023694c21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7bc8c19-0da9-4c54-9a9d-2fc1cf1c7ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mixpanel to Delta\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf6364f-160b-4105-b124-b566a8810d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\") \\\n",
    "    .option(\"multiLine\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"/Volumes/workspace/dashtoon_data/data/part-00000-tid-717492462703940002-1f7f4721-a439-4329-9e81-fd25cf4b9e97-412-1-c000.json.gz\")\n",
    "\n",
    "df = df.withColumn(\"event_date\", to_date(from_unixtime(col(\"time\"), \"yyyy-MM-dd\")))\n",
    "\n",
    "# Show result with all columns\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c1ddd0-587a-4606-913b-e39488fd3211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "# Extract year, month, and day from event_date column\n",
    "df = df.withColumn(\"year\", year(col(\"event_date\"))) \\\n",
    "       .withColumn(\"month\", month(col(\"event_date\"))) \\\n",
    "       .withColumn(\"day\", dayofmonth(col(\"event_date\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21eba855-21c9-4beb-bd3b-9e1f5022371f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c85ca69-f94a-4927-a34d-c4d80bb3006a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f2842e-0b03-4ecf-b24c-c4cecab5dcd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Column Name Cleaning Functions\n",
    "# Functions to clean column names with invalid characters\n",
    "def clean_column_name(name):\n",
    "    \"\"\"Replace invalid characters in column names with underscores\"\"\"\n",
    "    import re\n",
    "    # Replace invalid characters with underscores\n",
    "    return re.sub(r'[ ,;{}\\(\\)\\n\\t=]', '_', name)\n",
    "\n",
    "def clean_schema(schema, prefix=\"\"):\n",
    "    \"\"\"Recursively clean column names in a schema\"\"\"\n",
    "    clean_fields = []\n",
    "    \n",
    "    for field in schema.fields:\n",
    "        clean_name = clean_column_name(field.name)\n",
    "        \n",
    "        # Handle nested structures recursively\n",
    "        if isinstance(field.dataType, StructType):\n",
    "            clean_data_type = clean_schema(field.dataType, prefix + clean_name + \".\")\n",
    "            clean_field = StructField(clean_name, clean_data_type, field.nullable)\n",
    "        elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):\n",
    "            element_type = clean_schema(field.dataType.elementType, prefix + clean_name + \".\")\n",
    "            clean_data_type = ArrayType(element_type, field.dataType.containsNull)\n",
    "            clean_field = StructField(clean_name, clean_data_type, field.nullable)\n",
    "        else:\n",
    "            clean_field = StructField(clean_name, field.dataType, field.nullable)\n",
    "        \n",
    "        clean_fields.append(clean_field)\n",
    "    \n",
    "    return StructType(clean_fields)\n",
    "\n",
    "\n",
    "# Clean Top-Level Column Names\n",
    "print(\"Cleaning column names...\")\n",
    "# Create a clean schema\n",
    "clean_schema_struct = clean_schema(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b549d3-4e68-4f53-a62e-1337acb0f1c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_mappings = []\n",
    "for original_field, clean_field in zip(df.schema, clean_schema_struct):\n",
    "    if original_field.name != clean_field.name:\n",
    "        column_mappings.append((original_field.name, clean_field.name))\n",
    "        df = df.withColumnRenamed(original_field.name, clean_field.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85a4b3e1-c265-4e5d-8281-e2b5e64f7417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean Properties Struct and Handle Duplicates\n",
    "# For nested fields in properties struct\n",
    "if \"properties\" in [f.name for f in df.schema.fields]:\n",
    "    print(\"Found properties struct, cleaning nested column names...\")\n",
    "    \n",
    "    # Get all properties fields from schema\n",
    "    props_schema = [f for f in df.schema.fields if f.name == \"properties\"][0].dataType\n",
    "    \n",
    "    # First, print all property field names for debugging\n",
    "    print(\"All property fields:\")\n",
    "    for i, field in enumerate(props_schema.fields):\n",
    "        print(f\"  {i}: {field.name}\")\n",
    "    \n",
    "    # Create a map to track field names case-insensitively\n",
    "    field_name_map = {}\n",
    "    clean_props = []\n",
    "    \n",
    "    # First pass - catalog all field names and their cleaned versions\n",
    "    for i, field in enumerate(props_schema.fields):\n",
    "        original_name = field.name\n",
    "        clean_name = clean_column_name(original_name)\n",
    "        \n",
    "        # Track by lowercase name for case-insensitive deduplication\n",
    "        key = clean_name.lower()\n",
    "        \n",
    "        if key not in field_name_map:\n",
    "            field_name_map[key] = []\n",
    "        \n",
    "        field_name_map[key].append((i, original_name, clean_name))\n",
    "    \n",
    "    # Second pass - create expressions with unique names\n",
    "    for key, entries in field_name_map.items():\n",
    "        # If multiple entries with same name (case-insensitive), add suffixes\n",
    "        if len(entries) > 1:\n",
    "            print(f\"Found {len(entries)} duplicates for '{key}':\")\n",
    "            for idx, (i, original_name, _) in enumerate(entries):\n",
    "                unique_name = f\"{key}_{idx + 1}\"\n",
    "                print(f\"  Renaming: properties.`{original_name}` -> {unique_name}\")\n",
    "                clean_props.append(expr(f\"properties.`{original_name}`\").alias(unique_name))\n",
    "        else:\n",
    "            # Single entry - just use the cleaned name\n",
    "            i, original_name, clean_name = entries[0]\n",
    "            if original_name != clean_name:\n",
    "                print(f\"  Cleaning: properties.`{original_name}` -> {clean_name}\")\n",
    "            clean_props.append(expr(f\"properties.`{original_name}`\").alias(clean_name))\n",
    "    \n",
    "    # Create new properties struct with unique column names\n",
    "    print(f\"Creating new properties struct with {len(clean_props)} unique fields\")\n",
    "    df = df.withColumn(\"clean_properties\", struct(*clean_props))\n",
    "    df = df.drop(\"properties\").withColumnRenamed(\"clean_properties\", \"properties\")\n",
    "\n",
    "# Print a few rows to verify structure\n",
    "print(\"Sample data after cleaning:\")\n",
    "df.select(\"event_name\", \"properties.*\").show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8dcaa0e-2338-4470-be41-c871dd109e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_table_path = \"dbfs:/Volumes/workspace/dashtoon_data/data/mixpanel_events_data\"\n",
    "delta_table_name = \"workspace.dashtoon_data.mixpanel_events\"\n",
    "\n",
    "# delta_table_path = \"dbfs:/Volumes/workspace/dashtoon_data/data/incremental_data\"\n",
    "# delta_table_name = \"workspace.dashtoon_data.incremental_data\"\n",
    "\n",
    "write_options = {\n",
    "    \"format\": \"delta\",\n",
    "    \"partitionBy\": [\"year\", \"month\", \"day\"],\n",
    "    \"mode\": \"append\"\n",
    "}\n",
    "\n",
    "# Write Data to Delta Format inside the Volume\n",
    "print(\"Writing to Delta table...\")\n",
    "df.write \\\n",
    "    .format(write_options[\"format\"]) \\\n",
    "    .partitionBy(*write_options[\"partitionBy\"]) \\\n",
    "    .mode(write_options[\"mode\"]) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(delta_table_path)\n",
    "\n",
    "print(f\"Successfully processed Mixpanel data into Delta format at: {delta_table_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69f26b44-e87d-4b7a-a251-5821542512fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f950c180-450e-46f8-b30d-1fbe8fa8d244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c6f80d8-0248-4623-bd00-eb932de8e5a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Function to flatten the properties struct with better name handling\n",
    "def flatten_mixpanel_df(df):\n",
    "    \"\"\"\n",
    "    Flatten the properties struct in Mixpanel data with better unique column naming\n",
    "    \"\"\"\n",
    "    # Keep original top-level columns\n",
    "    base_columns = [col for col in df.columns if col != \"properties\"]\n",
    "    \n",
    "    # Generate column expressions with explicit aliases that preserve uniqueness\n",
    "    properties_cols = []\n",
    "    \n",
    "    # First, collect all field names to check for potential collisions\n",
    "    all_field_names = [field.name for field in df.schema[\"properties\"].dataType.fields]\n",
    "    \n",
    "    for field in df.schema[\"properties\"].dataType.fields:\n",
    "        field_name = field.name\n",
    "        \n",
    "        # Preserve dollar sign with underscore to maintain uniqueness\n",
    "        if field_name.startswith('$'):\n",
    "            column_name = f\"prop_dollar_{field_name[1:].replace('.', '_').replace('?', '_q_')}\"\n",
    "        elif field_name.startswith('payload.'):\n",
    "            column_name = f\"prop_payload_{field_name[8:].replace('.', '_').replace('?', '_q_')}\"\n",
    "        elif field_name.startswith('payload?'):\n",
    "            column_name = f\"prop_payload_q_{field_name[8:].replace('.', '_').replace('?', '_q_')}\"\n",
    "        else:\n",
    "            column_name = f\"prop_{field_name.replace('.', '_').replace('?', '_q_')}\"\n",
    "            \n",
    "        # Add original field to new dataframe with explicit alias\n",
    "        properties_cols.append(F.col(f\"properties.`{field_name}`\").alias(column_name))\n",
    "    \n",
    "    # Select all columns (base + flattened properties)\n",
    "    return df.select(*[F.col(c) for c in base_columns], *properties_cols)\n",
    "\n",
    "# Use the enhanced function\n",
    "flattened_df = flatten_mixpanel_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0414164d-2410-4ec9-8b95-3203d5366f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to Delta table\n",
    "flattened_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.dashtoon_data.flattened_mixpanel_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615a62df-0e1e-49f1-9d0c-b10aaa41c5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7806433-d40d-405d-9382-e9e24eec4b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load mixpanel_events DataFrame from Delta Table\n",
    "mixpanel_events_df = spark.read.format(\"delta\").load(\"dbfs:/Volumes/workspace/dashtoon_data/data/mixpanel_events_data\")\n",
    "\n",
    "# Register the DataFrame as a Temporary View to use in SQL Queries\n",
    "mixpanel_events_df.createOrReplaceTempView(\"mixpanel_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "509daab3-cd04-489c-b976-c5d87143ff71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#User Profile Query Db\n",
    "query = \"\"\"\n",
    "WITH subscription_events AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    time,\n",
    "    event_name AS sub_state,\n",
    "    ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY time DESC) AS rn\n",
    "  FROM mixpanel_events\n",
    "  WHERE event_name LIKE '%subscription%'\n",
    "),\n",
    "\n",
    "first_show_events AS (\n",
    "  SELECT DISTINCT \n",
    "    user_id,\n",
    "    properties.showName AS first_show,\n",
    "    ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY time) AS rn\n",
    "  FROM mixpanel_events\n",
    "  WHERE event_name IN ('showOpen', 'reelOpen')\n",
    "),\n",
    "\n",
    "appsflyer_data AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    properties.media_source AS media_source,\n",
    "    properties.campaign AS campaign,\n",
    "    properties.`payload.adset` AS adset\n",
    "  FROM mixpanel_events\n",
    "  WHERE event_name = 'appsFlyerInstall'\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  ue.user_id,\n",
    "  MIN(ue.distinct_id) AS distinct_id,\n",
    "  MAX(properties.email) AS user_email,\n",
    "  MAX(properties.`$os`) AS os,\n",
    "  MAX(properties.`$city`) AS city,\n",
    "  MAX(properties.`$model`) AS model,\n",
    "  MAX(properties.`$region`) AS country_code,\n",
    "  MAX(CASE WHEN af.adset IS NULL THEN properties.`payload.adset` ELSE af.adset END) AS adset,\n",
    "  MAX(CASE WHEN af.campaign IS NULL THEN properties.campaign ELSE af.campaign END) AS campaign,\n",
    "  MAX(CASE WHEN af.media_source IS NULL THEN properties.media_source ELSE af.media_source END) AS media_source,\n",
    "  MIN(ue.time) AS first_user_time,\n",
    "  MAX(fs.first_show) AS first_show,\n",
    "  MIN(CASE WHEN event_name = 'show1ActivatedLevel1' THEN properties.showName END) AS first_activated_show,\n",
    "  COUNT(DISTINCT CASE WHEN event_name = 'reelFinish' THEN CONCAT(properties.showId, properties.reelId) END) AS total_reels_seen,\n",
    "  COUNT(DISTINCT CASE WHEN event_name = 'reelFinish' THEN properties.showId END) AS total_show_seen,\n",
    "  COUNT(CASE WHEN event_name = 'adPaid' THEN ue.user_id END) AS total_ads_seen,\n",
    "  COUNT(DISTINCT sub.sub_state) AS is_subscriber,\n",
    "  COUNT(DISTINCT CASE WHEN sub.sub_state IN ('subscriptionPurchased', 'subscriptionRenewed') THEN ue.user_id END) AS is_active_subscriber\n",
    "FROM \n",
    "  mixpanel_events ue\n",
    "LEFT JOIN \n",
    "  subscription_events sub ON ue.user_id = sub.user_id AND sub.rn = 1\n",
    "LEFT JOIN \n",
    "  first_show_events fs ON ue.user_id = fs.user_id AND fs.rn = 1\n",
    "LEFT JOIN \n",
    "  appsflyer_data af ON ue.user_id = af.user_id\n",
    "GROUP BY \n",
    "  ue.user_id\n",
    "\"\"\"\n",
    "\n",
    "# Execute the Query\n",
    "user_profile_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c4aad1-870f-4277-939f-0c84febf83f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extra \n",
    "# Write to Delta table\n",
    "user_profile_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.dashtoon_data.new_user_profile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b888ce19-5d42-471f-92e0-5cb52c44b7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.dashtoon_data.new_user_profile limit 10 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bbac74f-13e6-4cef-8255-6f85e05d8c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE workspace.dashtoon_data.new_user_profile;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2fe4f8-960d-4242-9743-c18f0701f8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT DISTINCT event_name, properties.revenue\n",
    "FROM mixpanel_events\n",
    "WHERE event_name = 'adPaid'\n",
    "LIMIT 10;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d61447d-d6c0-4a37-81cf-2dc2386126e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ads arpu Db Query\n",
    "\n",
    "query=\"\"\"\n",
    "WITH ad_revenue AS (\n",
    "  SELECT \n",
    "    u.user_id,\n",
    "    DATE_TRUNC('DAY', FROM_UNIXTIME(u.first_user_time)) AS aq_timeline,\n",
    "    DATEDIFF(DAY, FROM_UNIXTIME(u.first_user_time), FROM_UNIXTIME(ue.time)) AS diff,\n",
    "    SUM(ue.properties.revenue) AS ads_revenue\n",
    "  FROM \n",
    "    workspace.dashtoon_data.new_user_profile u\n",
    "  LEFT JOIN \n",
    "    mixpanel_events ue ON u.user_id = ue.user_id\n",
    "  WHERE \n",
    "    ue.event_name = 'adPaid'\n",
    "  GROUP BY \n",
    "    1, 2, 3\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  aq_timeline,\n",
    "  ROUND(SUM(CASE WHEN diff = 0 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D0,\n",
    "  ROUND(SUM(CASE WHEN diff = 1 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D1,\n",
    "  ROUND(SUM(CASE WHEN diff = 2 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D2,\n",
    "  ROUND(SUM(CASE WHEN diff = 3 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D3,\n",
    "  ROUND(SUM(CASE WHEN diff = 4 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D4,\n",
    "  ROUND(SUM(CASE WHEN diff = 5 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D5,\n",
    "  ROUND(SUM(CASE WHEN diff = 6 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D6,\n",
    "  ROUND(SUM(CASE WHEN diff = 7 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D7,\n",
    "  ROUND(SUM(CASE WHEN diff = 14 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D14,\n",
    "  ROUND(SUM(CASE WHEN diff = 28 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D28,\n",
    "  ROUND(SUM(CASE WHEN diff = 60 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D60,\n",
    "  ROUND(SUM(CASE WHEN diff = 90 THEN ads_revenue END)/COUNT(DISTINCT user_id), 2) AS D90\n",
    "FROM \n",
    "  ad_revenue\n",
    "GROUP BY \n",
    "  1\n",
    "ORDER BY \n",
    "  1 DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad451f2-f279-4d88-bc1e-acb3d5558110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Average watch time Db Query\n",
    "\n",
    "query=\"\"\"\n",
    "WITH watch_time AS (\n",
    "  SELECT \n",
    "    u.user_id,\n",
    "    -- Option 1: Try using TO_DATE function\n",
    "    TO_DATE(FROM_UNIXTIME(u.first_user_time)) AS aq_timeline,    \n",
    "    DATEDIFF(DAY, FROM_UNIXTIME(u.first_user_time), FROM_UNIXTIME(ue.time)) AS diff,\n",
    "    SUM(CAST(ue.properties.timeSpent AS INT)) AS timeSpent\n",
    "  FROM \n",
    "    workspace.dashtoon_data.new_user_profile u\n",
    "  LEFT JOIN \n",
    "    mixpanel_events ue ON u.user_id = ue.user_id\n",
    "  WHERE \n",
    "    ue.event_name IN ('reelForegroundWatchTime')\n",
    "    AND u.first_user_time IS NOT NULL -- Make sure to filter out NULL values\n",
    "  GROUP BY \n",
    "    1, 2, 3\n",
    ")\n",
    "\n",
    "SELECT\n",
    "  aq_timeline,\n",
    "  ROUND(SUM(CASE WHEN diff = 0 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D0,\n",
    "  ROUND(SUM(CASE WHEN diff = 1 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D1,\n",
    "  ROUND(SUM(CASE WHEN diff = 2 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D2,\n",
    "  ROUND(SUM(CASE WHEN diff = 3 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D3,\n",
    "  ROUND(SUM(CASE WHEN diff = 4 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D4,\n",
    "  ROUND(SUM(CASE WHEN diff = 5 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D5,\n",
    "  ROUND(SUM(CASE WHEN diff = 6 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D6,\n",
    "  ROUND(SUM(CASE WHEN diff = 7 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D7,\n",
    "  ROUND(SUM(CASE WHEN diff = 14 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D14,\n",
    "  ROUND(SUM(CASE WHEN diff = 28 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D28,\n",
    "  ROUND(SUM(CASE WHEN diff = 60 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D60,\n",
    "  ROUND(SUM(CASE WHEN diff = 90 THEN timeSpent END)/NULLIF(COUNT(DISTINCT user_id), 0), 2) AS D90\n",
    "FROM \n",
    "  watch_time\n",
    "GROUP BY \n",
    "  1\n",
    "ORDER BY \n",
    "  1 DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97cc2f9-31d5-49c4-ae2d-fae4513be3e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crosswalk Show DB Query\n",
    "\n",
    "query=\"\"\"\n",
    "WITH show_2_activations AS (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    event_name,\n",
    "    properties.showName,\n",
    "    time\n",
    "  FROM \n",
    "    mixpanel_events\n",
    "),\n",
    "\n",
    "ft AS (\n",
    "  SELECT \n",
    "    u.first_activated_show AS show1,\n",
    "    s2.showName AS show2,\n",
    "    COUNT(DISTINCT s2.user_id) AS users\n",
    "  FROM \n",
    "    workspace.dashtoon_data.new_user_profile u\n",
    "  LEFT JOIN \n",
    "    show_2_activations s2 ON s2.user_id = u.user_id\n",
    "  WHERE \n",
    "    s2.showName IS NOT NULL\n",
    "    -- [[and u.country_code = {{country_code}}]]\n",
    "    -- [[and u.os = {{os}}]]\n",
    "  GROUP BY \n",
    "    1, 2\n",
    ")\n",
    "\n",
    "SELECT *\n",
    "FROM (\n",
    "  SELECT\n",
    "    show1 AS first_show_activated,\n",
    "    show2 AS top_cross_walk_shows,\n",
    "    ROW_NUMBER() OVER (PARTITION BY show1 ORDER BY users DESC) AS rn\n",
    "  FROM \n",
    "    ft\n",
    "  WHERE \n",
    "    show1 IS NOT NULL\n",
    "    -- [[and show1 = {{show_1}}]]\n",
    ")\n",
    "WHERE \n",
    "  rn <= 3\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f6fcde-a331-4592-ac8a-1f0e171ac4d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Overall Show details\n",
    "query=\"\"\"\n",
    "SELECT \n",
    "  ue.properties.showName,\n",
    "  ue.properties.showId,\n",
    "  COUNT(DISTINCT CASE WHEN ue.event_name = 'showOpen' THEN u.user_id END) AS showOpen_users,\n",
    "  COUNT(DISTINCT CASE WHEN ue.event_name = 'reelOpen' THEN u.user_id END) AS any_reelOpen_users,\n",
    "  COUNT(DISTINCT CASE WHEN ue.event_name = 'reelOpen' AND CAST(ue.properties.reelSequence AS INT) = 1 THEN u.user_id END) AS `1st_reelOpen_users`,\n",
    "  COUNT(DISTINCT CASE WHEN ue.event_name LIKE 'show%Activated' THEN u.user_id END) AS activated_users,\n",
    "  COUNT(DISTINCT CASE WHEN ue.event_name LIKE 'show%Activated' THEN u.user_id END) / \n",
    "    NULLIF(COUNT(DISTINCT CASE WHEN ue.event_name = 'showOpen' THEN u.user_id END), 0) AS `activation%`,\n",
    "  COUNT(DISTINCT CASE WHEN ue.event_name LIKE 'show%ActivatedLevel1' THEN u.user_id END) AS L1_activated_users,\n",
    "  COUNT(DISTINCT CASE WHEN ue.event_name LIKE 'show%ActivatedLevel1' THEN u.user_id END) / \n",
    "    NULLIF(COUNT(DISTINCT CASE WHEN ue.event_name = 'showOpen' THEN u.user_id END), 0) AS `L1_activation%`,\n",
    "  COUNT(DISTINCT CASE WHEN u.is_active_subscriber = 1 THEN u.user_id END) AS active_subscribers,\n",
    "  SUM(CASE WHEN ue.event_name = 'adPaid' THEN ue.properties.revenue END) AS ads_revenue\n",
    "FROM \n",
    "  workspace.dashtoon_data.new_user_profile u\n",
    "LEFT JOIN \n",
    "  mixpanel_events ue ON u.user_id = ue.user_id\n",
    "WHERE \n",
    "  TO_TIMESTAMP(u.first_user_time) >= TO_TIMESTAMP('2025-02-01')\n",
    "  AND ue.properties.showId IS NOT NULL\n",
    "  AND ue.properties.showName IS NOT NULL\n",
    "GROUP BY \n",
    "  ue.properties.showName,\n",
    "  ue.properties.showId\n",
    "ORDER BY \n",
    "  ue.properties.showName\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.show(10,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1688194929730378,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "delta_lake_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
